---
title: "P8105_hw3_yc384"
author: 'Ying Chen (UNI: yc384)'
date: "10/5/2019"
output: github_document
---

## P8105 DS HW 3

#### This homework includes figures; the readability embedded plots (e.g.font sizes, axis labels, titles)

### Problem 0
* created a public GitHub repo + local R Project named: p8105_hw3_yc384 
* created a single .Rmd file named p8105_hw3_yc384.Rmd that renders to github_document
* created a subdirectory to store the local data files used in Problems 1 and 2, and use relative paths to access these data files
* submit this link of my repo via Courseworks:
   https://github.com/YingCarolineChen/P8105_hw3_yc384.git

created a public GitHub repo and local R Project named
    p8105\_hw3\_yc384
  - created a .Rmd file named p8105\_hw3\_yc384.Rmd that renders to
    github\_document
  - created a subdirectory to store the local data files used in
    Problems 1 and 2, and use relative paths to access these data files
  - submit this link of my repo via Courseworks:
    <https://github.com/YingCarolineChen/P8105_hw3_yc384.git>

setwd(“/Users/macbook/Documents/Statistics/P8105/HW/P8105_HW3_yc384”)

#### We first setup our default R working enviroment

``` {r setup, include=TRUE}
library (tidyverse)
library (dplyr)
library(readxl)
library(patchwork)

# library for hw3 datasets 
library(p8105.datasets)

rm(list = ls())

knitr::opts_chunk$set(echo = TRUE, warning = TRUE,
  fig.width = 6,
  fig.asp = .6,
  out.width = "100%"
)
theme_set(theme_bw() + theme(legend.position = "bottom"))
options(tibble.print_min = 3)
```

### **Problem 1:** This problem uses the Instacart data.

#### 1-0 Import and tidy data for hw3-problem 1
For this homework we are using data from "The Instacart Online Grocery Shopping Dataset 2017". Instacart is an online grocery service that allows you to shop online from local stores. The original data is very extensive and we are only using a cleaned and limited version of the data. 

For varaible eval_set, all observation has a value of "train" exclusively. 
We will also rename some of the varaibles to make it more meaningful to readers. 

``` {r data1}
# Load dataset for problem 1 from library P8105.datasets
data("instacart")

# tidy data
instacart = 
  janitor::clean_names(instacart) %>% 
  # rename long varaibles
  rename(order_hr = order_hour_of_day, order_days = days_since_prior_order, order_weekday = order_dow) %>%
  # reorder data
  arrange(order_number, aisle_id) %>%
  select(aisle_id, product_name, order_number, order_hr, order_days, everything()) %>% 
  # remove duplicate rows
  distinct(instacart)
```

The instacart had 15 variables 1384617 oberserations. There were no duplicates and no observations were removed. Rows are items ordered by each customer and columns are other information related to that order. 


### 1-1 Perform EDA
#### 1-1 Write a short description of the dataset, noting the size and structure of the data, describing some key variables, and giving illstrative examples of observations.

```{r}
summary(instacart)
head(instacart)
```

**Answer:** We run a summary of the tidied dataset "instacart" that was loaded. There are 5 different ids: order_id, producti_id, user_id, aisle_id, department_id. Depend on the needs of the analysis, we may use different ids. From the data summary, we can see that there are 134 aisles, 21 Departments and 49688 products. The summary also tells us that 206209 users made 3421070 orders in total. The minimum order that a product 

From the first 5 lines of data, we can see that aisle one has food from deli department and has product of prepared soups salads. 


### 1-2 How many aisles are there, and which aisles are the most items ordered from?

```{r}
# number of aisles
instacart %>% 
  # group by aisles
  group_by(aisle_id) %>% 
  # count how often aisle appears
  count(aisle) %>% 
  # display number of rows in the table
  nrow()

# most selling aisle
instacart %>% 
  group_by(aisle_id) %>% 
  # count number of observations in each aisle
  summarize(
      n_order = n()) %>% 
  # sort aisle with highest sell on top
  mutate(rank = (rank(n_order))) %>%
  arrange(desc(rank)) %>% 
  # list top 5 aisles with most sells 
  top_n(5) %>% 
  # print table
  knitr::kable(caption = "Top 5 Selling Aisles", digits = 2)  
```

**Answer:** From the above result, we can see that there are 134 aisels. And the aile that had most items ordered from are 83, with total of 150609 items sold. And the 2nd one is aisle 24 with 150473 items sold. 


### 1-3 Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with > 10000 items ordered. Arrange aisles sensibly, and organize your plot so others can read it.

From the plots below, we can see that fresh vegetables and fresh fruits are the aisles that had the most orders from. Looks like people who are using this online ordering service are healthy eaters. 

```{r Plot_Order}
instacart %>%
 group_by(aisle) %>% 
  summarize(n_order = n()) %>%
  mutate(rank = rank(n_order)) %>% 
  arrange(desc(rank)) %>% 
  # plot scatterplot
  ggplot(aes(x = reorder(aisle, -rank), y = n_order)) +
   # plot points
   geom_bar(stat = "identity", aes(fill = n_order, show.legend = FALSE)) +
    labs(
      title = "Total Number of Items Ordered in Each Aisle (> 10K items)",
      x = "Aisle ID",
      y = "Total Number of Items Ordered",
      caption = "Data from p8105.datasets-instacart"
      ) 
```

### 1-4 Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.
```{r popular}
  instacart %>% 
  filter(aisle == "baking ingredients" | aisle == "dog food care" | 
         aisle == "packaged vegetables fruits") %>% 
  group_by(aisle, product_name) %>% 
  summarize(total = n()) %>% 
  group_by(aisle)%>% 
  filter(total == max(total)) %>%
  knitr::kable(caption = "Most Popular Item in the 3 Aisles")
```

**Answer:** From the results above, we can see that the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits” are: Light Brown Sugar from backing ingredients (499 orders), dog food care	Snack Sticks Chicken & Rice Recipe Dog Treats	(30 orders) and Organic Baby Spinach	(9784 orders). 

#### 1-5 Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e.produce a 2 x 7 table)
```{r day}
instacart %>% 
  filter(product_name == "Pink Lady Apples"|product_name == "Coffee Ice Cream") %>% 
  mutate(order_day = recode_factor(order_weekday,
         "0" = "Sunday",
         "1" = "Monday",
         "2" = "Tuesday",
         "3" = "Wednesday",
         "4" = "Thursday",
         "5" = "Friday",
         "6" = "Saturday",
         )) %>% 
  group_by(product_name, order_day) %>%
  summarize(mean = mean(order_hr)) %>% 
  spread(key = order_day, value = mean) %>% 
  knitr::kable(caption = "Mean hour of day Pink Lady Apples were ordered", digits = 2)
```

**Answer:** From the above results, we can see that everyday, customers always order apples around noon by lunch time and ice cream in the afternoon.


### **Problem 2:** This problem uses the BRFSS data.

For this homework we are using data from the Behavioral Risk Factors Surveillance System (BRFSS) for Selected Metropolitan Area Risk Trends (SMART) for 2002-2010. BRFSS data can be used to identify emerging health problems, establish and track health objectives, and develop and evaluate public health policies and programs. BRFSS is a continuous, state-based surveillance system that collects information about modifiable risk factors for chronic diseases and other leading causes of death.  

The dataset contains roughly 134,000 rows and 21 columns. There is information on lcoation, topic, question, response, and response number. The data is structured so that each (multiple-choice) response to each question is a separate row.

#### 2-1-0 Import data for hw3-problem 2

``` {r data2}
# Load dataset for problem 2 from library P8105.datasets
data("brfss_smart2010")
```

#### 2-1-0 We then look at the data to prepare for tidying

```{r}
summary(brfss_smart2010)
head(brfss_smart2010)
```

From above summary, we can see that there are many varaibles that have very long names and names for location difficult to know exact meaning. We will rename some of the varaibles in the code chunck below. 

### 2-1-1 Data cleaning: format the data to use appropriate variable names; focus on the “Overall Health” topic, include only responses from “Excellent” to “Poor”; organize responses as a factor taking levels ordered from “Poor” to “Excellent”

```{r tidy2}
brfss = 
  janitor::clean_names(brfss_smart2010) %>%   
  # rename long varaibles
  rename(state = locationabbr, location = locationdesc, CI_low = confidence_limit_low, CI_upper = confidence_limit_high) %>%
  # focus on the "Overall Health" topic
  filter(topic == "Overall Health") %>% 
  # recode response as a factor varaible
  mutate(
    response = forcats::fct_relevel(response, c("Excellent", "Very good", "Good", "Fair", "Poor"))
  ) %>% 
  # select only needed variables
  select(year, location, response, data_value) %>% 
  #organize responses ordered from “Poor” to “Excellent”
  arrange(desc(response))

# make sure response is sorted correctly
head(brfss, 3)
tail(brfss, 3)
```  
  
### 2-2 do or answer the following (commenting on the results of each):
#### 2-2-1 In 2002, which states were observed at 7 or more locations? What about in 2010?
```{r}
brfss %>% 
    separate(location, into = c("state", "county"), sep = "-", extra = "drop") %>%
    group_by(state, year) %>% 
    summarize(
      n_county = n()) %>% 
    filter(year == "2002", n_county >= 7) %>% 
    knitr::kable(caption = "States: 7 or more locations in 2002")
```

**Answer:**From above results, we can see that in 2002, there were 36 states that were observed at 7 or more locations.

```{r}
brfss %>% 
    separate(location, into = c("state", "county"), sep = "-", extra = "drop") %>%
    group_by(state, year) %>% 
    summarize(
      n_county = n()) %>% 
    filter(year == "2010", n_county >= 7) %>% 
    knitr::kable(caption = "States observed at 7 or more locations in 2002")
```

**Answer:**From above results, we can see that in 2010, there were 45 states that were observed at 7 or more locations. Much more than year 2002. 

#### 2-2-2 Construct a dataset that is limited to Excellent responses, and contains, year, state, and a variable that averages the data_value across locations within a state. 
```{r}
mean =
  brfss %>% 
    # remove missing data
    drop_na() %>% 
    # separated location into state and city/county
    separate(location, into = c("state", "county"), sep = "-", extra = "drop") %>%
    # selected only needed variables
    select(year, response, state, data_value) %>% 
    # limited only to Excellent responses
    filter(response == "Excellent") %>% 
    # group by state
    group_by(state, year) %>% 
    # take the mean of data_value
    summarize(
      mean = mean(data_value))
```

#### 2-2-2Make a “spaghetti” plot of this average value over time within a state (that is, make a plot showing a line for each state across years – the geom_line geometry and group aesthetic will help).
```{r}
mean %>% 
  group_by(state) %>% 
  ggplot ()+
  geom_line((aes(x = year, y = mean, group = state, color = state)))+
  ggtitle("Spaghetti Plot: Mean value of each state across years")
```

#### 2-3 Make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State.



### **Problem 3:** This problem uses the accel\_data. In this spreadsheet,

variables activity.\* are the activity counts for each minute of a
24-hour day starting at
midnight.

#### 3-1-1 Load, tidy, and otherwise wrangle the data. Dataset should include all originally observed variables and values; have useful variable names;

#### 3-1-2 Include a weekday vs weekend variable; and encode data with reasonable variable classes.

#### 3-1-3 Describe the resulting dataset (e.g. what variables exist, how many observations, etc).

#### 3-2-1 Using your tidied dataset, aggregate accross minutes to create a total activity variable for each day, and create a table showing these totals. Are any trends apparent?

#### 3-2-2 Make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week.

#### 3-2-3 Describe in words any patterns or conclusions you can make based on this graph.
